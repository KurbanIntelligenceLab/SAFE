# SAFE: A Sparse Autoencoder-Based Framework for Robust Query Enrichment and Hallucination Mitigation in LLMs

---

## ðŸ“– Overview
SAFE (Sparse Autoencoder-based Framework for Robust Query Enrichment) is a novel framework that **detects and mitigates hallucinations in Large Language Models (LLMs)** by leveraging Sparse Autoencoders (SAEs).  

The framework operates in two main stages:
1. **Hallucination Detection** â€“ plug-and-play detection using tools like SINdex, HaloCheck, or SelfCheckGPT.  
2. **Query Enrichment via SAEs** â€“ extracting interpretable features to refine queries and reduce misleading activations.  

SAFE consistently improves query accuracy and mitigates hallucinations across multiple datasets and models.

---

## âœ¨ Key Contributions
- **SAFE Framework**: A training-free, plug-and-play method to mitigate hallucinations in LLMs.  
- **Evaluation**: Comprehensive experiments on multiple QA benchmarks (TruthfulQA, BioASQ, WikiDoc, HaluEval).  
- **Performance**: Achieves accuracy improvements up to **29.45%** compared to baseline models.  
- **Open Access**: Publicly available code and resources to foster reproducibility and further research.  
 


